{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text(object):\n",
    "\n",
    "    def __init__(self,id,text,gene,mutation,category):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.gene = gene\n",
    "        self.mutation = mutation\n",
    "        self.category = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data (training_text,training_variants):\n",
    "\n",
    "    list_train = list();\n",
    "    with open (training_text, encoding='utf8') as file_input:\n",
    "        for line in file_input:\n",
    "            line = line.strip()\n",
    "            array_splits = line.split('||')\n",
    "            if len(array_splits)<2 :\n",
    "                continue\n",
    "            list_train.append(Text(array_splits[0],(array_splits[1]),\" \",\" \",\" \"))\n",
    "    #print(list_train)\n",
    "\n",
    "    i = 0\n",
    "    k = 0\n",
    "    with open(training_variants) as file_input:\n",
    "        for line in file_input:\n",
    "            line = line.strip()\n",
    "            k = k+1\n",
    "            if (k==1):\n",
    "                continue\n",
    "            array_splits = line.split(',')\n",
    "            list_train[i].gene = array_splits[1]\n",
    "            list_train[i].mutation = array_splits[2]\n",
    "            list_train[i].category = int(array_splits[3])-1\n",
    "            i = i+1\n",
    "\n",
    "    return list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#keras imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, Flatten, LSTM, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "#scikit learn imports\n",
    "from keras.preprocessing.text import one_hot\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#general python imports\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found word vectors:  999995\n"
     ]
    }
   ],
   "source": [
    "dim = 300\n",
    "e_index = {}\n",
    "f = open('/Users/shyla/Desktop/Dataset/wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for line in f:\n",
    "    text = line.rstrip().rsplit(' ', dim)\n",
    "    word = text[0]\n",
    "    coefs = np.asarray(text[1:], dtype='float32')\n",
    "    e_index[word] = coefs\n",
    "f.close()\n",
    "print('Found word vectors: ', len(e_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Reading File\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4 5 6 7 8], y=[0, 1, 1, 2, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 4, 3, 0, 3, 4, 3, 5, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 6, 3, 1, 6, 3, 3, 0, 0, 3, 3, 0, 3, 0, 3, 0, 3, 0, 3, 3, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 3, 0, 0, 3, 6, 6, 6, 0, 1, 0, 3, 0, 1, 1, 6, 6, 6, 1, 1, 6, 6, 1, 6, 6, 6, 0, 3, 0, 3, 3, 3, 0, 3, 3, 1, 3, 3, 0, 0, 3, 0, 3, 0, 4, 0, 0, 4, 0, 4, 3, 3, 3, 0, 0, 3, 0, 0, 3, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 6, 1, 4, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 1, 6, 1, 4, 1, 6, 1, 6, 1, 1, 1, 1, 3, 6, 6, 6, 6, 1, 6, 6, 1, 1, 6, 6, 6, 6, 1, 1, 1, 6, 1, 1, 1, 6, 1, 1, 6, 6, 6, 6, 5, 1, 6, 6, 6, 2, 6, 6, 3, 6, 1, 6, 6, 6, 1, 6, 6, 1, 1, 6, 6, 6, 1, 6, 1, 1, 3, 6, 1, 7, 6, 1, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 1, 1, 3, 6, 6, 1, 6, 6, 6, 1, 6, 1, 0, 6, 6, 6, 6, 4, 1, 6, 1, 1, 6, 1, 6, 6, 6, 1, 6, 6, 1, 1, 1, 6, 1, 1, 6, 6, 0, 0, 6, 6, 0, 0, 3, 5, 3, 2, 0, 3, 3, 6, 6, 1, 1, 2, 1, 2, 7, 7, 7, 0, 0, 4, 4, 0, 4, 1, 1, 1, 1, 6, 1, 6, 1, 1, 1, 1, 1, 6, 5, 1, 1, 6, 6, 1, 1, 1, 6, 1, 1, 1, 6, 0, 0, 0, 3, 3, 3, 0, 3, 0, 0, 3, 1, 3, 3, 3, 6, 3, 6, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 0, 3, 3, 0, 0, 0, 3, 3, 0, 3, 3, 0, 5, 3, 3, 0, 0, 3, 1, 3, 0, 3, 3, 3, 0, 3, 0, 3, 0, 0, 5, 0, 3, 3, 0, 3, 0, 0, 0, 5, 0, 3, 0, 0, 0, 3, 0, 0, 3, 3, 0, 0, 0, 3, 0, 3, 0, 3, 3, 3, 0, 3, 3, 0, 0, 0, 0, 3, 3, 0, 0, 3, 3, 0, 4, 0, 0, 3, 0, 3, 0, 3, 3, 3, 0, 3, 0, 0, 3, 0, 0, 3, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 2, 3, 3, 3, 3, 2, 0, 0, 1, 3, 0, 0, 3, 0, 3, 0, 3, 0, 0, 3, 3, 5, 0, 1, 0, 4, 0, 0, 3, 0, 8, 0, 3, 0, 0, 3, 3, 0, 0, 3, 3, 0, 5, 0, 0, 3, 3, 3, 0, 0, 0, 0, 3, 0, 4, 0, 4, 3, 0, 5, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 3, 3, 3, 3, 3, 2, 3, 3, 5, 0, 3, 0, 0, 4, 3, 0, 0, 3, 3, 5, 3, 4, 5, 5, 0, 0, 3, 3, 3, 0, 0, 0, 3, 3, 0, 0, 3, 3, 3, 0, 6, 6, 6, 1, 3, 6, 3, 2, 3, 4, 3, 0, 3, 3, 0, 3, 0, 0, 1, 3, 0, 4, 3, 0, 3, 6, 0, 0, 5, 3, 0, 5, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 5, 4, 1, 1, 1, 6, 4, 5, 6, 5, 6, 5, 5, 5, 5, 6, 6, 6, 1, 5, 5, 6, 6, 6, 1, 3, 6, 4, 4, 6, 1, 5, 3, 1, 0, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 1, 6, 7, 6, 3, 1, 6, 5, 3, 6, 6, 1, 4, 6, 6, 1, 4, 5, 1, 3, 6, 6, 1, 6, 6, 5, 6, 6, 4, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 2, 2, 6, 6, 5, 6, 1, 2, 5, 5, 5, 6, 6, 6, 6, 6, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 3, 5, 3, 0, 0, 3, 3, 0, 3, 0, 3, 3, 0, 3, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 6, 6, 6, 1, 0, 0, 0, 6, 1, 4, 6, 4, 6, 2, 6, 6, 6, 6, 6, 2, 6, 6, 2, 6, 4, 6, 6, 2, 2, 6, 2, 6, 1, 6, 2, 6, 6, 6, 6, 6, 6, 2, 2, 6, 6, 1, 6, 6, 6, 0, 6, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 0, 1, 1, 1, 1, 1, 1, 1, 6, 1, 5, 0, 1, 1, 5, 1, 1, 1, 6, 1, 6, 3, 0, 0, 3, 3, 3, 3, 1, 3, 1, 2, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 1, 6, 3, 0, 1, 3, 0, 3, 6, 6, 3, 3, 4, 3, 3, 0, 2, 2, 2, 3, 0, 3, 0, 2, 3, 3, 2, 3, 3, 2, 3, 0, 0, 3, 0, 3, 0, 0, 0, 3, 3, 3, 3, 0, 3, 3, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 3, 0, 0, 3, 3, 0, 6, 3, 0, 3, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 1, 6, 6, 6, 1, 6, 6, 1, 1, 1, 6, 6, 1, 6, 0, 0, 0, 1, 0, 0, 6, 0, 6, 0, 8, 8, 8, 8, 8, 8, 6, 6, 6, 1, 5, 5, 0, 5, 4, 0, 0, 0, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 0, 3, 3, 3, 0, 6, 5, 1, 6, 6, 5, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 1, 6, 6, 1, 6, 1, 6, 6, 6, 1, 1, 6, 6, 6, 6, 6, 6, 8, 8, 8, 4, 3, 0, 0, 3, 0, 0, 0, 0, 0, 6, 4, 1, 6, 6, 6, 6, 5, 6, 6, 6, 1, 5, 6, 1, 6, 4, 5, 6, 6, 1, 4, 1, 6, 6, 1, 6, 6, 6, 4, 6, 6, 6, 4, 1, 6, 6, 6, 6, 4, 1, 1, 6, 1, 1, 4, 1, 6, 6, 6, 1, 1, 6, 4, 6, 6, 1, 6, 1, 1, 1, 6, 6, 1, 1, 6, 0, 0, 1, 5, 5, 5, 5, 4, 6, 6, 6, 1, 6, 1, 1, 3, 3, 3, 0, 0, 0, 3, 3, 0, 0, 3, 3, 0, 3, 3, 3, 3, 3, 0, 0, 3, 0, 3, 3, 6, 5, 1, 6, 0, 4, 6, 4, 0, 6, 1, 1, 6, 6, 1, 1, 1, 1, 6, 6, 1, 1, 6, 1, 1, 1, 6, 1, 1, 1, 1, 0, 0, 3, 5, 0, 5, 3, 5, 0, 4, 5, 5, 3, 4, 0, 3, 0, 3, 4, 4, 5, 5, 3, 0, 3, 4, 0, 3, 5, 5, 4, 5, 0, 5, 3, 5, 5, 5, 0, 1, 1, 6, 2, 6, 7, 2, 6, 1, 6, 6, 6, 6, 7, 6, 6, 6, 4, 4, 6, 4, 6, 6, 6, 6, 6, 6, 2, 1, 6, 8, 6, 6, 6, 8, 1, 6, 6, 6, 6, 6, 4, 6, 6, 0, 6, 6, 6, 1, 6, 6, 6, 6, 1, 4, 6, 0, 1, 6, 6, 6, 6, 1, 1, 6, 4, 1, 6, 5, 4, 0, 6, 4, 4, 4, 6, 6, 4, 4, 6, 6, 4, 6, 6, 6, 6, 1, 1, 6, 4, 1, 6, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 0, 3, 3, 3, 0, 0, 3, 3, 0, 0, 0, 6, 6, 6, 1, 4, 1, 4, 0, 5, 6, 6, 6, 0, 6, 6, 5, 6, 6, 6, 6, 6, 5, 1, 6, 0, 6, 6, 5, 6, 1, 6, 0, 4, 6, 6, 6, 1, 6, 6, 0, 6, 4, 1, 6, 6, 6, 1, 4, 6, 6, 6, 0, 0, 6, 6, 6, 4, 6, 6, 1, 6, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 4, 1, 4, 6, 6, 6, 6, 6, 1, 6, 6, 6, 2, 6, 6, 6, 2, 2, 2, 2, 6, 6, 6, 1, 6, 2, 6, 4, 2, 2, 6, 6, 6, 6, 6, 1, 6, 4, 3, 3, 3, 6, 1, 1, 6, 1, 3, 0, 0, 0, 0, 3, 0, 3, 0, 3, 5, 4, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 3, 0, 0, 3, 3, 0, 3, 3, 3, 3, 0, 0, 3, 3, 0, 4, 3, 3, 3, 3, 0, 3, 0, 3, 3, 3, 3, 3, 0, 3, 1, 3, 3, 6, 5, 6, 6, 6, 6, 6, 6, 2, 2, 6, 6, 2, 6, 6, 6, 6, 6, 6, 1, 6, 1, 2, 6, 1, 1, 6, 2, 2, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 4, 4, 3, 3, 0, 0, 0, 0, 3, 2, 3, 4, 3, 4, 4, 0, 3, 6, 6, 0, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 3, 0, 3, 0, 6, 3, 3, 0, 3, 4, 3, 3, 3, 0, 4, 5, 3, 3, 3, 3, 4, 0, 0, 3, 3, 3, 6, 8, 8, 3, 8, 8, 7, 4, 3, 4, 4, 4, 4, 3, 3, 4, 8, 8, 7, 8, 7, 5, 1, 0, 0, 4, 0, 0, 0, 0, 3, 4, 6, 0, 5, 5, 6, 0, 6, 6, 5, 6, 6, 6, 6, 4, 1, 4, 4, 6, 5, 6, 6, 1, 6, 6, 6, 6, 6, 1, 3, 0, 3, 1, 3, 0, 6, 3, 3, 1, 6, 1, 3, 1, 0, 7, 0, 6, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 3, 0, 3, 1, 1, 0, 3, 3, 3, 3, 0, 0, 0, 8, 0, 1, 6, 6, 2, 6, 6, 1, 6, 2, 6, 4, 2, 6, 6, 6, 6, 6, 1, 6, 2, 2, 6, 2, 6, 6, 6, 6, 1, 6, 6, 6, 6, 2, 1, 1, 2, 1, 6, 6, 2, 6, 6, 6, 2, 2, 6, 0, 6, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 4, 1, 6, 1, 1, 1, 1, 1, 1, 6, 1, 6, 6, 6, 6, 5, 6, 1, 6, 1, 5, 6, 6, 6, 1, 1, 6, 1, 6, 6, 6, 6, 6, 3, 6, 3, 3, 3, 1, 0, 0, 0, 0, 3, 3, 4, 2, 0, 0, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 5, 6, 6, 6, 1, 6, 4, 6, 6, 6, 6, 1, 3, 6, 6, 6, 4, 5, 6, 6, 6, 1, 6, 6, 5, 6, 1, 5, 4, 6, 1, 6, 1, 6, 6, 6, 4, 6, 1, 6, 6, 6, 5, 1, 1, 5, 6, 6, 6, 6, 5, 6, 6, 0, 6, 6, 6, 6, 1, 6, 6, 1, 1, 6, 5, 1, 6, 6, 6, 6, 1, 3, 0, 6, 3, 3, 2, 0, 5, 3, 4, 4, 0, 0, 0, 5, 5, 5, 5, 5, 0, 4, 6, 6, 6, 0, 0, 0, 1, 0, 5, 0, 0, 0, 3, 4, 0, 3, 3, 0, 3, 3, 0, 0, 1, 1, 3, 3, 3, 3, 0, 0, 6, 1, 0, 1, 3, 0, 6, 1, 6, 1, 6, 6, 6, 6, 6, 6, 1, 1, 6, 6, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 0, 3, 0, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 6, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 7, 7, 0, 7, 7, 0, 0, 0, 0, 0, 8, 0, 6, 6, 6, 6, 6, 1, 1, 1, 5, 3, 3, 3, 0, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 1, 6, 5, 5, 1, 1, 5, 5, 6, 6, 5, 6, 5, 6, 6, 5, 5, 6, 5, 5, 5, 6, 6, 5, 6, 6, 5, 6, 6, 5, 6, 6, 1, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 4, 6, 3, 6, 6, 6, 1, 6, 1, 1, 5, 6, 6, 0, 6, 3, 6, 6, 1, 7, 6, 6, 5, 6, 6, 6, 6, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 5, 0, 0, 0, 0, 3, 5, 3, 3, 0, 0, 0, 4, 0, 5, 3, 5, 5, 2, 5, 0, 0, 5, 5, 5, 0, 5, 0, 0, 4, 0, 4, 0, 5, 5, 4, 4, 3, 5, 5, 5, 3, 5, 2, 0, 3, 5, 2, 3, 4, 5, 3, 0, 4, 0, 4, 4, 4, 2, 0, 4, 4, 5, 0, 0, 5, 3, 0, 3, 4, 2, 0, 0, 5, 5, 0, 4, 4, 2, 3, 0, 3, 4, 4, 4, 4, 3, 4, 2, 4, 0, 4, 4, 5, 3, 0, 2, 5, 2, 0, 3, 4, 0, 5, 5, 4, 5, 3, 4, 5, 5, 0, 0, 0, 0, 5, 4, 4, 3, 3, 4, 4, 4, 3, 4, 4, 3, 4, 4, 5, 4, 0, 3, 3, 5, 4, 4, 3, 4, 5, 2, 0, 4, 0, 4, 3, 0, 4, 3, 5, 4, 3, 3, 5, 4, 4, 0, 4, 0, 3, 4, 5, 3, 3, 4, 3, 0, 0, 0, 2, 0, 0, 0, 3, 4, 0, 2, 5, 3, 5, 2, 0, 5, 5, 3, 0, 5, 0, 0, 0, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 0, 4, 4, 3, 4, 0, 4, 3, 3, 3, 5, 4, 4, 3, 4, 4, 4, 3, 2, 5, 0, 5, 4, 3, 4, 0, 4, 4, 3, 4, 5, 0, 5, 4, 3, 4, 0, 4, 0, 0, 3, 4, 4, 3, 5, 0, 4, 4, 4, 4, 0, 5, 4, 4, 4, 3, 0, 4, 5, 4, 5, 4, 2, 0, 4, 4, 5, 6, 5, 6, 3, 6, 6, 6, 1, 6, 4, 5, 1, 6, 1, 6, 6, 4, 6, 3, 4, 1, 4, 6, 1, 1, 1, 6, 6, 1, 6, 1, 1, 6, 1, 6, 1, 1, 6, 6, 6, 6, 1, 6, 1, 6, 1, 2, 6, 6, 1, 1, 4, 6, 1, 6, 4, 6, 1, 1, 5, 4, 1, 6, 6, 1, 6, 1, 6, 6, 1, 1, 6, 1, 6, 6, 6, 6, 1, 6, 6, 1, 6, 6, 1, 6, 6, 6, 6, 1, 6, 6, 6, 4, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5, 4, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 0, 0, 0, 5, 4, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 0, 0, 5, 5, 0, 3, 5, 4, 5, 0, 5, 3, 5, 5, 5, 5, 4, 5, 4, 5, 0, 4, 5, 4, 5, 0, 0, 5, 5, 5, 5, 4, 5, 4, 5, 5, 3, 4, 5, 5, 5, 5, 4, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 0, 5, 0, 0, 0, 0, 0, 0, 4, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 1, 6, 6, 0, 0, 1, 6, 3, 6, 1, 1, 6, 6, 6, 1, 5, 5, 6, 6, 1, 0, 0, 6, 6, 1, 1, 6, 6, 0, 0, 1, 1, 6, 6, 1, 6, 1, 6, 2, 6, 6, 1, 6, 6, 1, 1, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 1, 6, 1, 1, 1, 1, 6, 6, 6, 1, 1, 6, 1, 6, 1, 6, 1, 6, 6, 6, 1, 1, 6, 6, 6, 6, 1, 1, 1, 1, 1, 6, 5, 1, 6, 6, 6, 1, 1, 6, 6, 1, 6, 1, 1, 6, 6, 1, 6, 6, 1, 6, 6, 1, 1, 1, 1, 6, 6, 6, 1, 3, 6, 1, 6, 6, 1, 6, 6, 1, 1, 1, 1, 1, 6, 8, 1, 1, 0, 6, 1, 6, 6, 1, 6, 6, 1, 1, 6, 6, 6, 1, 0, 6, 4, 1, 6, 6, 1, 1, 1, 3, 6, 6, 6, 1, 6, 1, 1, 1, 0, 1, 6, 6, 6, 6, 6, 6, 6, 1, 0, 0, 1, 7, 0, 0, 0, 2, 7, 0, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 5, 4, 5, 6, 5, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 5, 6, 6, 1, 6, 6, 1, 6, 0, 0, 1, 6, 6, 6, 1, 6, 6, 6, 6, 1, 6, 6, 6, 6, 2, 0, 0, 6, 6, 6, 1, 3, 0, 5, 5, 1, 6, 1, 6, 6, 6, 6, 6, 1, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 4, 4, 0, 0, 0, 0, 1, 6, 3, 6, 1, 1, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 0, 1, 4, 6, 6, 1, 6, 1, 5, 6, 5, 6, 6, 6, 6, 5, 1, 5, 1, 1, 6, 0, 0, 3, 2, 3, 6, 6, 6, 4, 6, 6, 6, 6, 6, 1, 4, 3, 1, 1, 6, 3, 1, 3, 1, 3, 4, 6, 6, 1, 1, 4, 1, 6, 1, 6, 1, 6, 4, 1, 6, 6, 0, 6, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 6, 0, 6, 3, 3, 3, 5, 3, 3, 0, 0, 3, 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "training_variants_filename = \"/Users/shyla/Desktop/Dataset/training_variants\"\n",
    "training_text_filename = \"/Users/shyla/Desktop/Dataset/training_text\"\n",
    "training_list = read_training_data(training_text_filename,training_variants_filename)\n",
    "print (\"Done Reading File\")\n",
    "#CNN_LEARN(training_list)\n",
    "\n",
    "text_train = [];\n",
    "score_train = [];\n",
    "for text in training_list:\n",
    "    text_train.append(text.text);\n",
    "    score_train.append(int(text.category));\n",
    "    \n",
    "vocab_size = 5000\n",
    "max_len = 500\n",
    "    \n",
    "texts = text_train\n",
    "myclass_weight = class_weight.compute_class_weight('balanced', np.unique(score_train), score_train)\n",
    "score_train = to_categorical(score_train, num_classes=9)\n",
    "MAX_NO_WORDS = 1000\n",
    "    \n",
    "#this will restrict the index only top 1000 words in document\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "    \n",
    "#fit the tokenizer on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#convert the sentecess in text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "encoded_text_train = [one_hot(d, vocab_size) for d in texts]\n",
    "    \n",
    "#check the number of unique tokense\n",
    "#word_index = tokenizer.word_index\n",
    "#print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#keep the sequence lengths of fixed size, if > then truncate, else pad with 0\n",
    "#MAX_SEQUENCE_LENGTH = 300\n",
    "data = pad_sequences(encoded_text_train, maxlen=max_len)\n",
    "#assume x_train, x_test, y_train, y_test variables contain the required data\n",
    "#populate the data accordingly.\n",
    "\n",
    "#load the word embeddings from the pretrained word vectors and get a dictionary\n",
    "#embeddings_index = {}\n",
    "#embedding_vector_length = 100\n",
    "\n",
    "#split the data into training and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, score_train, test_size = 0.2, random_state = 42, stratify=score_train)\n",
    "\n",
    "#load pretrained word model\n",
    "#w2v = gensim.models.Word2Vec.load('/Users/shyla/Desktop/Dataset/w2vmodeladdeddata').wv\n",
    "#word_index = w2v.word_index\n",
    "#number_found = 0\n",
    "#number_not_found = 0\n",
    "\n",
    "    \n",
    "\n",
    "#create the emeddinig matrix based on the words in vocab in and the embdedding vecrtors\n",
    "#embedding_matrix = np.zeros((len(word_index) + 1, embedding_vector_length))\n",
    "#for word, i in word_index.items():\n",
    "    #if word in w2v.key_to_index:\n",
    "        #embedding_vector = w2v[word]\n",
    "        #embedding_matrix[i] = embedding_vector\n",
    "        #number_found += 1\n",
    "        #continue\n",
    "    #number_not_found += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202946, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_matrix = np.zeros((len(word_index) + 1, dim))\n",
    "for word, i in word_index.items():\n",
    "    vector = e_index.get(word)\n",
    "    if vector is not None:\n",
    "        e_matrix[i] = vector\n",
    "len(e_matrix)\n",
    "e_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('embedding-2.h5', 'w') as hf:\n",
    "    hf.create_dataset('fasttext', data=e_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202946, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "with h5py.File('embedding-2.h5', 'r') as hf:\n",
    "    mat = hf['fasttext'][:]\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          60883800  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 497, 100)          120100    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 494, 100)          40100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 164, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 161, 100)          40100     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 158, 100)          40100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 52, 100)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 52, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5200)              0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 100)               520100    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 909       \n",
      "=================================================================\n",
      "Total params: 61,645,209\n",
      "Trainable params: 761,409\n",
      "Non-trainable params: 60,883,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shyla\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 1.9218 - categorical_crossentropy: 1.9218 - acc: 0.2459\n",
      "Epoch 2/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 1.8422 - categorical_crossentropy: 1.8422 - acc: 0.2903\n",
      "Epoch 3/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 1.7295 - categorical_crossentropy: 1.7295 - acc: 0.3479\n",
      "Epoch 4/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 1.6065 - categorical_crossentropy: 1.6065 - acc: 0.4153\n",
      "Epoch 5/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 1.4354 - categorical_crossentropy: 1.4354 - acc: 0.4985\n",
      "Epoch 6/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 1.2869 - categorical_crossentropy: 1.2869 - acc: 0.5546\n",
      "Epoch 7/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 1.1771 - categorical_crossentropy: 1.1771 - acc: 0.5873\n",
      "Epoch 8/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 1.0492 - categorical_crossentropy: 1.0492 - acc: 0.6404\n",
      "Epoch 9/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 0.9913 - categorical_crossentropy: 0.9913 - acc: 0.6687\n",
      "Epoch 10/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 0.9430 - categorical_crossentropy: 0.9430 - acc: 0.6819\n",
      "Epoch 11/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 0.8953 - categorical_crossentropy: 0.8953 - acc: 0.6973\n",
      "Epoch 12/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 0.8458 - categorical_crossentropy: 0.8458 - acc: 0.7191\n",
      "Epoch 13/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 0.7838 - categorical_crossentropy: 0.7838 - acc: 0.7225\n",
      "Epoch 14/20\n",
      "2656/2656 [==============================] - 13s 5ms/step - loss: 0.7843 - categorical_crossentropy: 0.7843 - acc: 0.7274\n",
      "Epoch 15/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 0.7369 - categorical_crossentropy: 0.7369 - acc: 0.7406\n",
      "Epoch 16/20\n",
      "2656/2656 [==============================] - 12s 4ms/step - loss: 0.7230 - categorical_crossentropy: 0.7230 - acc: 0.7500\n",
      "Epoch 17/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 0.7113 - categorical_crossentropy: 0.7113 - acc: 0.7451\n",
      "Epoch 18/20\n",
      "2656/2656 [==============================] - 12s 4ms/step - loss: 0.6622 - categorical_crossentropy: 0.6622 - acc: 0.7617\n",
      "Epoch 19/20\n",
      "2656/2656 [==============================] - 11s 4ms/step - loss: 0.6727 - categorical_crossentropy: 0.6727 - acc: 0.7515\n",
      "Epoch 20/20\n",
      "2656/2656 [==============================] - 11s 4ms/step - loss: 0.6679 - categorical_crossentropy: 0.6679 - acc: 0.7628\n",
      "Accuracy: 157.68%\n",
      "Log loss: 1.5767780941395926\n",
      "Accuracy: 0.5368421052631579\n"
     ]
    }
   ],
   "source": [
    "#create an embedding layer with the embeddings created above\n",
    "embedding_layer = Embedding(len(word_index) + 1, 300, weights=[e_matrix], input_length = max_len, trainable=False)\n",
    "##print(x_train.shape)\n",
    "#print('Number of embeddings found: {}'.format(number_found))\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "#This below model is a cnn architecture implemented according to the Medical Text Classification using CNN paper\n",
    "#model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_len))\n",
    "\n",
    "#model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_len))\n",
    "\n",
    "#This is the CNN architecture that gave the best results. It has two sets of two 1D Conv layers followed by a MaxPooling Layer\n",
    "#The number of fileters is restricted to 100 and two dropout layers are used to prevent overfitting\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=100, kernel_size=4, activation='relu'))\n",
    "model.add(Conv1D(filters=100, kernel_size=4, activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(filters=100, kernel_size=4, activation='relu'))\n",
    "\n",
    "model.add(Conv1D(filters=100, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', name='dense1'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_crossentropy','acc'])\n",
    "\n",
    "print(model.summary())\n",
    "#print(y_train.shape)\n",
    "\n",
    "#Trained the built model on the training data\n",
    "model.fit(x_train, y_train, nb_epoch=20, batch_size=64, class_weight=myclass_weight)\n",
    "\n",
    "#Evaluate the scores of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "probas = model.predict(x_test)\n",
    "pred_indices = np.argmax(probas, axis=1)\n",
    "classes = np.array(range(0,9))\n",
    "preds = classes[pred_indices]\n",
    "#model.save('../models/cnn_model4.h5')\n",
    "print('Log loss: {}'.format(log_loss(classes[np.argmax(y_test, axis=1)], probas)))\n",
    "print('Accuracy: {}'.format(accuracy_score(classes[np.argmax(y_test, axis=1)], preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import html\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "# Use these if using python 2.7\n",
    "# import sys\n",
    "# import codecs\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    ")\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "#import of scipy\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "from tempfile import TemporaryFile\n",
    "import pandas as pd\n",
    "#import cPickle\n",
    "#import theano as th\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "#set this if using theano backend\n",
    "#os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from keras import initializers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text(object):\n",
    "\n",
    "    def __init__(self,id,text,gene,mutation,category):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.gene = gene\n",
    "        self.mutation = mutation\n",
    "        self.category = category\n",
    "\n",
    "def read_training_data (training_text,training_variants):\n",
    "\n",
    "    list_train = list();\n",
    "    k=0\n",
    "    with codecs.open (training_text,'r','utf8') as file_input:\n",
    "        for line in file_input:\n",
    "            line = line.strip()\n",
    "            array_splits = line.split('||')\n",
    "            k = k+1\n",
    "            if k==1:\n",
    "                print(array_splits)\n",
    "            if len(array_splits)<2 :\n",
    "                continue\n",
    "            list_train.append(Text(array_splits[0],preprocess_string(array_splits[1]),\" \",\" \",\" \"))\n",
    "\n",
    "    i = 0\n",
    "    k = 0\n",
    "    print(len(list_train))\n",
    "    with codecs.open(training_variants,'r','utf8') as file_input:\n",
    "        for line in file_input:\n",
    "            line = line.strip()\n",
    "            k = k+1\n",
    "            if (k==1):\n",
    "                continue\n",
    "            array_splits = line.split(',')\n",
    "            if len(array_splits)<4 :\n",
    "                continue\n",
    "            list_train[i].gene = array_splits[1]\n",
    "            list_train[i].mutation = array_splits[2]\n",
    "            list_train[i].category = int(array_splits[3])-1\n",
    "            i = i+1\n",
    "\n",
    "    return list_train\n",
    "def preprocess_string(string):\n",
    "    \n",
    "    string = html.unescape(string)\n",
    "    string = string.replace(\"\\\\n\",\" \")\n",
    "    string = string.replace(\"_NEG\",\"\")\n",
    "    string = string.replace(\"_NEGFIRST\", \"\")\n",
    "    string = re.sub(r\"@[A-Za-z0-9_s(),!?\\'\\`]+\", \"\", string) # removing any twitter handle mentions\n",
    "    string = re.sub(r\"#\", \"\", string)\n",
    "    string = re.sub(r\"\\*\", \"\", string)\n",
    "    string = re.sub(r\"\\'s\", \"\", string)\n",
    "    string = re.sub(r\"\\'m\", \" am\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" have\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" not\", string)\n",
    "    string = re.sub(r\"\\'re\", \" are\", string)\n",
    "    string = re.sub(r\"\\'d\", \" would\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" will\", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" !\", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" ?\", string)\n",
    "    string = re.sub(r'[^\\x00-\\x7F]',' ', string)\n",
    "    # pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    # string  = pattern.sub(' ',string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string #return remove_stopwords(string.strip().lower())\n",
    "    \n",
    "def remove_stopwords(string):\n",
    "    split_string = \\\n",
    "    [word for word in string.split()\n",
    "        if word not in stopwords.words('english')]\n",
    "\n",
    "    return \" \".join(split_string)\n",
    "\n",
    "\n",
    "def LSTM_first_model(training_list):\n",
    "\n",
    "    text_train = list();\n",
    "    score_train = list();\n",
    "    for text in training_list:\n",
    "        text_train.append(text.text);\n",
    "        score_train.append(int(text.category));\n",
    "\n",
    "    score_train = to_categorical(score_train, num_classes=9)\n",
    "    vocab_size = 500000\n",
    "    max_len = 2000\n",
    "\n",
    "    encoded_text_train\t= [one_hot(d, vocab_size) for d in text_train]\n",
    "    padded_train \t= pad_sequences(encoded_text_train,maxlen = max_len,padding = 'post')\n",
    "    #Embedding Layes\n",
    "    embedding_1 = Embedding(vocab_size,100, input_length=max_len)\n",
    "\n",
    "\n",
    "    #LSTM Layers\n",
    "    lstm_1 = LSTM(256, dropout=0.2, recurrent_dropout=0.2, name='lstm1', return_sequences=True)\n",
    "    lstm_2 = LSTM(128, dropout=0.2, recurrent_dropout=0.2, name='lstm2', return_sequences=True)\n",
    "    lstm_3 = LSTM(16, dropout=0.2, recurrent_dropout=0.2, name='lstm3',return_sequences = True)\n",
    "    #Dense Layers\n",
    "    dense_1 = Dense(128, activation='relu', name='dense1')\n",
    "    dense_2 = Dense(9, activation='sigmoid', name='dense2')\n",
    "\n",
    "    def get_model():\n",
    "        model = Sequential()\n",
    "        model.add(embedding_1)\n",
    "        # model.add(bi_lstm_1)\n",
    "        # model.add(conv_1)\n",
    "        # model.add(conv_2)\n",
    "        # model.add(lstm_1)\n",
    "        # model.add(lstm_2)\n",
    "        model.add(lstm_3)\n",
    "        model.add(Flatten())\n",
    "        #model.add(dense_1)\n",
    "        model.add(dense_2)\n",
    "        #compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['categorical_crossentropy','acc'])\n",
    "        # summarize the model\n",
    "        print(model.summary())\n",
    "        # fit the model\n",
    "        return model\n",
    "\n",
    "    #create the model\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(padded_train, score_train, test_size = 0.2, random_state = 42, stratify=score_train)\n",
    "    estimator = get_model()\n",
    "    estimator.fit(X_train,Y_train,nb_epoch =5,batch_size=512,verbose=1)\n",
    "    estimator.save('my_model_lstm_1.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "    probas = estimator.predict(X_test)\n",
    "    pred_indices = np.argmax(probas, axis=1)\n",
    "    classes = np.array(range(0,9))\n",
    "    preds = classes[pred_indices]\n",
    "    print('Log loss: {}'.format(log_loss(classes[np.argmax(Y_test, axis=1)], probas)))\n",
    "    print('Accuracy: {}'.format(accuracy_score(classes[np.argmax(Y_test, axis=1)], preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lstm_glove(training_list,test_list):\n",
    "    \n",
    "    docs_train = list()\n",
    "    score_train = list()\n",
    "    total_dataset = list()\n",
    "    for text in training_list:\n",
    "        docs_train.append(text.text);\n",
    "        score_train.append(int(text.category));\n",
    "        total_dataset.append(text.text)\n",
    "    \n",
    "    score_train = to_categorical(score_train, num_classes=9)\n",
    "    docs_test = list();\n",
    "    # score_test = list();\n",
    "    for text in test_list:\n",
    "        docs_test.append(text.text);\n",
    "    #    score_test.append(int(text.category));\n",
    "        total_dataset.append(text.text)\n",
    "#    print (total_daatset)\n",
    "\n",
    "#    score_test = to_categorical(score_test, nb_classes=9)\n",
    "    t = Tokenizer(lower=False)\n",
    "    t.fit_on_texts(total_dataset)\n",
    "    word_index = t.word_index\n",
    "    print(t.document_count)\n",
    "    vocab_size = len(t.word_counts)\n",
    "    print(vocab_size)\n",
    "    print (len(word_index))\n",
    "    max_len = 300\n",
    "    sequences_train = t.texts_to_sequences(docs_train)\n",
    "    sequences_test = t.texts_to_sequences(docs_test)\n",
    "    \n",
    "    dim = 300\n",
    "    e_matrix = np.zeros((len(word_index) + 1, dim))\n",
    "    for word, i in word_index.items():\n",
    "        vector = e_index.get(word)\n",
    "        if vector is not None:\n",
    "            e_matrix[i] = vector\n",
    "    len(e_matrix)\n",
    "    e_matrix.shape\n",
    "    \n",
    "    #EMBEDDING_DIM = 100\n",
    "    #glove_train = gensim.models.Word2Vec.load('/Users/shyla/Desktop/Dataset/w2vmodeladdeddata').wv  # Be sure to have this dict in the same folder as the code\n",
    "    # np.load('./Data/embeddingmap.npy',encoding = 'latin1');\n",
    "    #glove_test = np.load('./Data/embeddingmap_test_wvec.npy',encoding = 'latin1')\n",
    "#    print(len(glove_train))\n",
    "    #print(len(glove_test.item()))\n",
    "\n",
    "    #embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    #number_found =0\n",
    "    #number_not_found = 0\n",
    "    #embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "#    print(glove_train.item().keys())\n",
    "    #for word, i in word_index.items():\n",
    "#        \n",
    "        #if word in glove_train.key_to_index:\n",
    "            \n",
    "          #embedding_vector = glove_train[word]\n",
    "          #embedding_matrix[i] = embedding_vector\n",
    "          #number_found +=1\n",
    "          #continue\n",
    "#        print('matched')\n",
    "#        print(word)\n",
    "         \n",
    "        #number_not_found+=1\n",
    "#        print('no match')\n",
    "#        print(word)\n",
    "\n",
    "    #print(number_found)\n",
    "    #print(number_not_found)\n",
    "    \n",
    "    padded_train     = pad_sequences(sequences_train,maxlen = max_len,padding = 'post')\n",
    "    padded_test        = pad_sequences(sequences_test,maxlen = max_len,padding = 'post')\n",
    "\n",
    "\n",
    "    embedding_1 = Embedding(len(word_index) + 1,\n",
    "                                dim,weights=[e_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=False)\n",
    "\n",
    "    #LSTM Layers\n",
    "    lstm_1 = LSTM(256, dropout=0.2, recurrent_dropout=0.2, name='lstm1', return_sequences=True)\n",
    "    lstm_2 = LSTM(128, dropout=0.2, recurrent_dropout=0.2, name='lstm2', return_sequences=True)\n",
    "    lstm_3 = LSTM(64, dropout=0.2, recurrent_dropout=0.2, name='lstm3')\n",
    "    lstm_4 = LSTM(32, dropout=0.2, recurrent_dropout=0.2, name='lstm4',return_sequences = True)\n",
    "\n",
    "    #Dense Layers\n",
    "    dense_1 = Dense(200, activation='relu', name='dense1')\n",
    "    dense_2 = Dense(9, activation='softmax', name='dense2')\n",
    "\n",
    "    def get_model():\n",
    "        model = Sequential()\n",
    "        model.add(embedding_1)\n",
    "#        model.add(lstm_1)\n",
    "        model.add(lstm_2)\n",
    "        model.add(lstm_3)\n",
    "        model.add(dense_1)\n",
    "        model.add(dense_2)\n",
    "        #compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['categorical_crossentropy','acc'])\n",
    "        # summarize the model\n",
    "        print(model.summary())\n",
    "        # fit the model\n",
    "        return model\n",
    "\n",
    "    #create the model\n",
    "    estimator = get_model()\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(padded_train,score_train,test_size=0.20, random_state=42)\n",
    "    estimator.fit(data_train,labels_train,epochs =10,validation_data = (data_test,labels_test),batch_size=128,verbose=1)\n",
    "    estimator.save('my_model_lstm_10_full.h5')  \n",
    "    probas = estimator.predict(data_test)\n",
    "    pred_indices = np.argmax(probas, axis=1)\n",
    "    classes = np.array(range(0,9))\n",
    "    preds = classes[pred_indices]\n",
    "\n",
    "    estimator.fit(data_train,labels_train,validation_data = (data_test,labels_test),epochs=10,batch_size = 128,verbose=1)\n",
    "    estimator.save('model_20_full.h5')\n",
    "    probas = estimator.predict(data_test)\n",
    "    pred_indices = np.argmax(probas, axis=1)\n",
    "    classes = np.array(range(0,9))\n",
    "    preds = classes[pred_indices]\n",
    "    #print('Log loss: {}'.format(log_loss(labels_test, probas)))\n",
    "    #print('Accuracy: {}'.format(accuracy_score(labels_test, preds)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lstm_wev_hierarchial(training_list,test_list):\n",
    "\n",
    "    from nltk import tokenize\n",
    "    \n",
    "    train_documents = []\n",
    "    train_labels = []\n",
    "    train_texts = []\n",
    "\n",
    "    test_documents = []\n",
    "    test_texts = []\n",
    "    \n",
    "    total_documents = []\n",
    "    total_texts = []\n",
    "\n",
    "    for idx in range(len(training_list)):\n",
    "        text = (training_list[idx].text)\n",
    "        train_texts.append(text)\n",
    "        total_texts.append(text)\n",
    "\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        \n",
    "        train_documents.append(sentences)\n",
    "        train_labels.append(int(training_list[idx].category))\n",
    "        total_documents.append(sentences)\n",
    "\n",
    "    for idx in range(len(test_list)):\n",
    "        text = (test_list[idx].text)\n",
    "        test_texts.append(text)\n",
    "        total_texts.append(text)\n",
    "\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        \n",
    "        test_documents.append(sentences)\n",
    "        total_documents.append(sentences)\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(total_texts)\n",
    "\n",
    "    MAX_SENTS = 100\n",
    "    MAX_SENT_LENGTH = 20\n",
    "    data = np.zeros((len(train_texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "    word_index = tokenizer.word_index\n",
    "    for i, sentences in enumerate(train_documents):\n",
    "        for j, sent in enumerate(sentences):\n",
    "            if j< MAX_SENTS:\n",
    "                wordTokens = text_to_word_sequence(sent)\n",
    "                k=0\n",
    "                for _, word in enumerate(wordTokens):\n",
    "                    if k<MAX_SENT_LENGTH:\n",
    "                        data[i,j,k] = tokenizer.word_index[word]\n",
    "                        k=k+1                    \n",
    "                        \n",
    "    vocab_size = len(word_index)\n",
    "    labels = to_categorical(np.asarray(train_labels))\n",
    "\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    dim = 300\n",
    "    e_matrix = np.zeros((len(word_index) + 1, dim))\n",
    "    for word, i in word_index.items():\n",
    "        vector = e_index.get(word)\n",
    "        if vector is not None:\n",
    "            e_matrix[i] = vector\n",
    "    len(e_matrix)\n",
    "    e_matrix.shape\n",
    "\n",
    "    def get_model():\n",
    "        input_sent = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "        embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                     dim,weights=[e_matrix],\n",
    "                                     input_length=MAX_SENT_LENGTH,\n",
    "                                     trainable=False)\n",
    "        emb_1 = embedding_layer(input_sent)\n",
    "        l_lstm = Bidirectional(LSTM(100)) (emb_1)\n",
    "        encoded_sentence = Model(input_sent, l_lstm)\n",
    "\n",
    "        input_document = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "        encoded_document = TimeDistributed(encoded_sentence)(input_document)\n",
    "        l_lstm_sent = Bidirectional(LSTM(100))(encoded_document)\n",
    "        pen_ultimate = Dense(100,activation='sigmoid')(l_lstm_sent)\n",
    "        preds = Dense(9, activation='softmax')(pen_ultimate)\n",
    "        model = Model(input_document, preds)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['categorical_crossentropy','acc'])\n",
    "        return model        \n",
    "\n",
    "    #create the model\n",
    "    estimator = get_model()\n",
    "    \n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.20, random_state=42)\n",
    "    estimator.fit(data_train,labels_train,validation_data = (data_test,labels_test), epochs =10,batch_size=64,verbose=1)\n",
    "    estimator.save('model_10_hierarchial.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "    estimator.fit(data_train,labels_train,validation_data = (data_test,labels_test), epochs =10,batch_size=64,verbose=1)\n",
    "    estimator.save('model_20_hierarchial.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shyla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID,Text']\n",
      "3321\n",
      "['ID,Text']\n",
      "986\n",
      "Done Reading File\n",
      "4307\n",
      "199320\n",
      "199320\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 300)          59796300  \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 300, 128)          219648    \n",
      "_________________________________________________________________\n",
      "lstm3 (LSTM)                 (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 200)               13000     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 9)                 1809      \n",
      "=================================================================\n",
      "Total params: 60,080,165\n",
      "Trainable params: 283,865\n",
      "Non-trainable params: 59,796,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2656 samples, validate on 665 samples\n",
      "Epoch 1/10\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.9646 - categorical_crossentropy: 1.9646 - acc: 0.2609 - val_loss: 1.8261 - val_categorical_crossentropy: 1.8261 - val_acc: 0.2917\n",
      "Epoch 2/10\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 1.8468 - categorical_crossentropy: 1.8468 - acc: 0.2858 - val_loss: 1.8062 - val_categorical_crossentropy: 1.8062 - val_acc: 0.2917\n",
      "Epoch 3/10\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 1.8331 - categorical_crossentropy: 1.8331 - acc: 0.2858 - val_loss: 1.7951 - val_categorical_crossentropy: 1.7951 - val_acc: 0.2917\n",
      "Epoch 4/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.8033 - categorical_crossentropy: 1.8033 - acc: 0.3136 - val_loss: 1.7263 - val_categorical_crossentropy: 1.7263 - val_acc: 0.3263\n",
      "Epoch 5/10\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.7476 - categorical_crossentropy: 1.7476 - acc: 0.3426 - val_loss: 1.6179 - val_categorical_crossentropy: 1.6179 - val_acc: 0.3744\n",
      "Epoch 6/10\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.6548 - categorical_crossentropy: 1.6548 - acc: 0.3938 - val_loss: 1.5537 - val_categorical_crossentropy: 1.5537 - val_acc: 0.4271\n",
      "Epoch 7/10\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 1.5630 - categorical_crossentropy: 1.5630 - acc: 0.4273 - val_loss: 1.4833 - val_categorical_crossentropy: 1.4833 - val_acc: 0.4346\n",
      "Epoch 8/10\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.5267 - categorical_crossentropy: 1.5267 - acc: 0.4435 - val_loss: 1.4736 - val_categorical_crossentropy: 1.4736 - val_acc: 0.4331\n",
      "Epoch 9/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.4857 - categorical_crossentropy: 1.4857 - acc: 0.4571 - val_loss: 1.4534 - val_categorical_crossentropy: 1.4534 - val_acc: 0.4421\n",
      "Epoch 10/10\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.4524 - categorical_crossentropy: 1.4524 - acc: 0.4770 - val_loss: 1.4167 - val_categorical_crossentropy: 1.4167 - val_acc: 0.4617\n",
      "Train on 2656 samples, validate on 665 samples\n",
      "Epoch 1/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.4043 - categorical_crossentropy: 1.4043 - acc: 0.4838 - val_loss: 1.4245 - val_categorical_crossentropy: 1.4245 - val_acc: 0.4647\n",
      "Epoch 2/10\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.3593 - categorical_crossentropy: 1.3593 - acc: 0.5120 - val_loss: 1.3800 - val_categorical_crossentropy: 1.3800 - val_acc: 0.4887\n",
      "Epoch 3/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.3493 - categorical_crossentropy: 1.3493 - acc: 0.4989 - val_loss: 1.3709 - val_categorical_crossentropy: 1.3709 - val_acc: 0.4782\n",
      "Epoch 4/10\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.3105 - categorical_crossentropy: 1.3105 - acc: 0.5169 - val_loss: 1.3336 - val_categorical_crossentropy: 1.3336 - val_acc: 0.5008\n",
      "Epoch 5/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.2631 - categorical_crossentropy: 1.2631 - acc: 0.5365 - val_loss: 1.3118 - val_categorical_crossentropy: 1.3118 - val_acc: 0.5128\n",
      "Epoch 6/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.2492 - categorical_crossentropy: 1.2492 - acc: 0.5399 - val_loss: 1.2886 - val_categorical_crossentropy: 1.2886 - val_acc: 0.5233\n",
      "Epoch 7/10\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.2130 - categorical_crossentropy: 1.2130 - acc: 0.5610 - val_loss: 1.3066 - val_categorical_crossentropy: 1.3066 - val_acc: 0.5053\n",
      "Epoch 8/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.1933 - categorical_crossentropy: 1.1933 - acc: 0.5614 - val_loss: 1.2784 - val_categorical_crossentropy: 1.2784 - val_acc: 0.5308\n",
      "Epoch 9/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.1575 - categorical_crossentropy: 1.1575 - acc: 0.5761 - val_loss: 1.2669 - val_categorical_crossentropy: 1.2669 - val_acc: 0.5263\n",
      "Epoch 10/10\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.1288 - categorical_crossentropy: 1.1288 - acc: 0.5892 - val_loss: 1.2776 - val_categorical_crossentropy: 1.2776 - val_acc: 0.5459\n",
      "Train on 2656 samples, validate on 665 samples\n",
      "Epoch 1/10\n",
      "2656/2656 [==============================] - 138s 52ms/step - loss: 1.8761 - categorical_crossentropy: 1.8761 - acc: 0.2899 - val_loss: 1.8566 - val_categorical_crossentropy: 1.8566 - val_acc: 0.2737\n",
      "Epoch 2/10\n",
      "2656/2656 [==============================] - 137s 51ms/step - loss: 1.8335 - categorical_crossentropy: 1.8335 - acc: 0.2854 - val_loss: 1.8633 - val_categorical_crossentropy: 1.8633 - val_acc: 0.2737\n",
      "Epoch 3/10\n",
      "2656/2656 [==============================] - 134s 50ms/step - loss: 1.7405 - categorical_crossentropy: 1.7405 - acc: 0.3434 - val_loss: 1.6588 - val_categorical_crossentropy: 1.6588 - val_acc: 0.3729\n",
      "Epoch 4/10\n",
      "2656/2656 [==============================] - 137s 52ms/step - loss: 1.5259 - categorical_crossentropy: 1.5259 - acc: 0.4477 - val_loss: 1.4574 - val_categorical_crossentropy: 1.4574 - val_acc: 0.4872\n",
      "Epoch 5/10\n",
      "2656/2656 [==============================] - 138s 52ms/step - loss: 1.3980 - categorical_crossentropy: 1.3980 - acc: 0.5049 - val_loss: 1.4249 - val_categorical_crossentropy: 1.4249 - val_acc: 0.4737\n",
      "Epoch 6/10\n",
      "2656/2656 [==============================] - 147s 55ms/step - loss: 1.3173 - categorical_crossentropy: 1.3173 - acc: 0.5241 - val_loss: 1.4028 - val_categorical_crossentropy: 1.4028 - val_acc: 0.4917\n",
      "Epoch 7/10\n",
      "2656/2656 [==============================] - 143s 54ms/step - loss: 1.2536 - categorical_crossentropy: 1.2536 - acc: 0.5437 - val_loss: 1.3394 - val_categorical_crossentropy: 1.3394 - val_acc: 0.5038\n",
      "Epoch 8/10\n",
      "2656/2656 [==============================] - 141s 53ms/step - loss: 1.1865 - categorical_crossentropy: 1.1865 - acc: 0.5745 - val_loss: 1.3157 - val_categorical_crossentropy: 1.3157 - val_acc: 0.5203\n",
      "Epoch 9/10\n",
      "2656/2656 [==============================] - 141s 53ms/step - loss: 1.1704 - categorical_crossentropy: 1.1704 - acc: 0.5855 - val_loss: 1.3401 - val_categorical_crossentropy: 1.3401 - val_acc: 0.5293\n",
      "Epoch 10/10\n",
      "2656/2656 [==============================] - 138s 52ms/step - loss: 1.1002 - categorical_crossentropy: 1.1002 - acc: 0.6096 - val_loss: 1.2764 - val_categorical_crossentropy: 1.2764 - val_acc: 0.5429\n",
      "Train on 2656 samples, validate on 665 samples\n",
      "Epoch 1/10\n",
      "2656/2656 [==============================] - 140s 53ms/step - loss: 1.0278 - categorical_crossentropy: 1.0278 - acc: 0.6329 - val_loss: 1.2861 - val_categorical_crossentropy: 1.2861 - val_acc: 0.5459\n",
      "Epoch 2/10\n",
      "2656/2656 [==============================] - 142s 54ms/step - loss: 0.9652 - categorical_crossentropy: 0.9652 - acc: 0.6581 - val_loss: 1.2490 - val_categorical_crossentropy: 1.2490 - val_acc: 0.5534\n",
      "Epoch 3/10\n",
      "2656/2656 [==============================] - 142s 54ms/step - loss: 0.9323 - categorical_crossentropy: 0.9323 - acc: 0.6717 - val_loss: 1.2669 - val_categorical_crossentropy: 1.2669 - val_acc: 0.5429\n",
      "Epoch 4/10\n",
      "2656/2656 [==============================] - 140s 53ms/step - loss: 0.8900 - categorical_crossentropy: 0.8900 - acc: 0.6916 - val_loss: 1.2980 - val_categorical_crossentropy: 1.2980 - val_acc: 0.5218\n",
      "Epoch 5/10\n",
      "2656/2656 [==============================] - 145s 55ms/step - loss: 0.8634 - categorical_crossentropy: 0.8634 - acc: 0.6792 - val_loss: 1.2880 - val_categorical_crossentropy: 1.2880 - val_acc: 0.5414\n",
      "Epoch 6/10\n",
      "2656/2656 [==============================] - 139s 52ms/step - loss: 0.8011 - categorical_crossentropy: 0.8011 - acc: 0.7139 - val_loss: 1.3109 - val_categorical_crossentropy: 1.3109 - val_acc: 0.5429\n",
      "Epoch 7/10\n",
      "2656/2656 [==============================] - 135s 51ms/step - loss: 0.7711 - categorical_crossentropy: 0.7711 - acc: 0.7278 - val_loss: 1.3197 - val_categorical_crossentropy: 1.3197 - val_acc: 0.5504\n",
      "Epoch 8/10\n",
      "2656/2656 [==============================] - 140s 53ms/step - loss: 0.7554 - categorical_crossentropy: 0.7554 - acc: 0.7278 - val_loss: 1.3002 - val_categorical_crossentropy: 1.3002 - val_acc: 0.5624\n",
      "Epoch 9/10\n",
      "2656/2656 [==============================] - 131s 49ms/step - loss: 0.7270 - categorical_crossentropy: 0.7270 - acc: 0.7346 - val_loss: 1.3015 - val_categorical_crossentropy: 1.3015 - val_acc: 0.5729\n",
      "Epoch 10/10\n",
      "2656/2656 [==============================] - 132s 50ms/step - loss: 0.6769 - categorical_crossentropy: 0.6769 - acc: 0.7605 - val_loss: 1.3428 - val_categorical_crossentropy: 1.3428 - val_acc: 0.5684\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "training_variants_filename = \"/Users/shyla/Desktop/Dataset/training_variants\"\n",
    "training_text_filename = \"/Users/shyla/Desktop/Dataset/training_text\"\n",
    "\n",
    "test_filename = \"/Users/shyla/Desktop/Dataset/stages/stage2_test_text.csv\"\n",
    "test_variant = \"/Users/shyla/Desktop/Dataset/stages/stage2_test_variants.csv\"\n",
    "\n",
    "training_list = read_training_data(training_text_filename,training_variants_filename)\n",
    "test_list = read_training_data(test_filename,test_variant)\n",
    "print (\"Done Reading File\")\n",
    "embedding_lstm_glove(training_list,test_list)\n",
    "embedding_lstm_wev_hierarchial(training_list,test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
